{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "Copy of Copy of feature_less_transformation2git.ipynb",
   "provenance": [],
   "collapsed_sections": [
    "iOUuyVwsdI8O",
    "OrDaW_48dgpz",
    "06idvN1Zdm5y",
    "XLvcs7NIdWWX",
    "fn0JDn5Qe30m",
    "D-sS3KEYe93d",
    "Wkv-5f-MfDd-",
    "QTy_5LMFf0HY"
   ]
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "iOUuyVwsdI8O"
   },
   "source": [
    "# Configs"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "t1G6G2-Eohhp"
   },
   "source": [
    "import copy\n",
    "import os\n",
    "import random\n",
    "import time\n",
    "from collections import defaultdict\n",
    "\n",
    "import matplotlib.image as image\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pydicom\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "from scipy.special import comb\n",
    "from skimage import io\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import transforms"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "try:\n",
    "    os.mkdir(\"../log_dir/ss\")\n",
    "    os.mkdir(\"../models/ss_models\")\n",
    "except FileExistsError:\n",
    "    pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2DEAw3a1cgvq",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "9c712f42-ef97-4b85-fc00-faba7884a4c7"
   },
   "source": [
    "class Config:\n",
    "    def __init__(self,\n",
    "                 data_augmentation=True,\n",
    "                 nonlinear_rate=0.5,\n",
    "                 paint_rate=0.6,\n",
    "                 outpaint_rate=0.4,\n",
    "                 flip_rate=0.5,\n",
    "                 local_rate=0.4,\n",
    "                 load_saved_model_path='../models/pretrained/ss_pretrained.pth.tar',\n",
    "                 original_dataset_dir='../data/Dataset',\n",
    "                 preprocessed_dataset_dir='../data/Preprocessed_Dataset/',\n",
    "                 labels_file='../data/labels.csv',\n",
    "                 label_type='PVWM',\n",
    "                 preprocess=True,\n",
    "                 begin_at_slice=6,\n",
    "                 num_slices=7,\n",
    "                 image_crop_size=128\n",
    "                 ):\n",
    "        self.data_augmentation = data_augmentation\n",
    "        self.nonlinear_rate = nonlinear_rate\n",
    "        self.paint_rate = paint_rate\n",
    "        self.outpaint_rate = outpaint_rate\n",
    "        self.inpaint_rate = 1.0 - self.outpaint_rate\n",
    "        self.flip_rate = flip_rate\n",
    "        self.local_rate = local_rate\n",
    "        self.load_saved_model_path = load_saved_model_path\n",
    "        self.original_dataset_dir = original_dataset_dir\n",
    "        self.preprocessed_dataset_dir = preprocessed_dataset_dir\n",
    "        self.labels_file = labels_file\n",
    "        self.label_type = label_type\n",
    "        self.preprocess = preprocess\n",
    "        self.begin_at_slice = begin_at_slice\n",
    "        self.num_slices = num_slices\n",
    "        self.image_crop_size = image_crop_size\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Display Configuration values.\"\"\"\n",
    "        print(\"\\nConfigurations:\")\n",
    "        for a in dir(self):\n",
    "            if not a.startswith(\"__\") and not callable(getattr(self, a)):\n",
    "                print(\"{:30} {}\".format(a, getattr(self, a)))\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "config = Config()\n",
    "config.display()"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OrDaW_48dgpz"
   },
   "source": [
    "## Augmentations"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mEaSktRfchgr"
   },
   "source": [
    "def bernstein_poly(i, n, t):\n",
    "    \"\"\"\n",
    "     The Bernstein polynomial of n, i as a function of t\n",
    "    \"\"\"\n",
    "    return comb(n, i) * (t ** (n - i)) * (1 - t) ** i\n",
    "\n",
    "\n",
    "def bezier_curve(points, nTimes=1000):\n",
    "    \"\"\"\n",
    "       Given a set of control points, return the\n",
    "       bezier curve defined by the control points.\n",
    "\n",
    "       Control points should be a list of lists, or list of tuples\n",
    "       such as [ [1,1], \n",
    "                 [2,3], \n",
    "                 [4,5], ..[Xn, Yn] ]\n",
    "        nTimes is the number of time steps, defaults to 1000\n",
    "\n",
    "        See http://processingjs.nihongoresources.com/bezierinfo/\n",
    "    \"\"\"\n",
    "\n",
    "    nPoints = len(points)\n",
    "    xPoints = np.array([p[0] for p in points])\n",
    "    yPoints = np.array([p[1] for p in points])\n",
    "\n",
    "    t = np.linspace(0.0, 1.0, nTimes)\n",
    "\n",
    "    polynomial_array = np.array([bernstein_poly(i, nPoints - 1, t) for i in range(0, nPoints)])\n",
    "    xvals = np.dot(xPoints, polynomial_array)\n",
    "    yvals = np.dot(yPoints, polynomial_array)\n",
    "    return xvals, yvals\n",
    "\n",
    "\n",
    "def data_augmentation(x, y, prob=0.5):\n",
    "    # augmentation by flipping\n",
    "    cnt = 1\n",
    "    while random.random() < prob and cnt > 0:\n",
    "        degree = random.choice([0, 1])\n",
    "        # print('augmentation')\n",
    "        x = np.flip(x, axis=degree)\n",
    "        y = np.flip(y, axis=degree)\n",
    "        cnt = cnt - 1\n",
    "    return x, y\n",
    "\n",
    "\n",
    "def nonlinear_transformation(x, prob=0.5):\n",
    "    if random.random() >= prob:\n",
    "        return x\n",
    "    points = [[0, 0], [random.random(), random.random()], [random.random(), random.random()], [1, 1]]\n",
    "    xpoints = [p[0] for p in points]\n",
    "    ypoints = [p[1] for p in points]\n",
    "    xvals, yvals = bezier_curve(points, nTimes=100)\n",
    "    if random.random() < 0.5:\n",
    "        # Half change to get flip\n",
    "        xvals = np.sort(xvals)\n",
    "    else:\n",
    "        xvals, yvals = np.sort(xvals), np.sort(yvals)\n",
    "    nonlinear_x = 255 * np.interp(np.true_divide(x, 255), xvals, yvals)\n",
    "    # print('nonlinear_transformation')\n",
    "    return nonlinear_x\n",
    "\n",
    "\n",
    "def local_pixel_shuffling(x, prob=0.5):\n",
    "    # print(x)\n",
    "    if random.random() >= prob:\n",
    "        return x\n",
    "    image_temp = copy.deepcopy(x)\n",
    "    orig_image = copy.deepcopy(x)\n",
    "    img_deps, img_rows, img_cols = x.shape\n",
    "    num_block = 800\n",
    "    for _ in range(num_block):\n",
    "        block_noise_size_x = random.randint(1, img_rows // 20)\n",
    "        block_noise_size_y = random.randint(1, img_cols // 20)\n",
    "        # block_noise_size_z = random.randint(1, img_deps//10)\n",
    "        noise_x = random.randint(0, img_rows - block_noise_size_x)\n",
    "        noise_y = random.randint(0, img_cols - block_noise_size_y)\n",
    "        # noise_z = random.randint(0, img_deps-block_noise_size_z)\n",
    "        window = orig_image[0, noise_x:noise_x + block_noise_size_x,\n",
    "                 noise_y:noise_y + block_noise_size_y]\n",
    "        # print(window)\n",
    "        window = window.flatten()\n",
    "        np.random.shuffle(window)\n",
    "        window = window.reshape((1, block_noise_size_x,\n",
    "                                 block_noise_size_y))\n",
    "        image_temp[0, noise_x:noise_x + block_noise_size_x,\n",
    "        noise_y:noise_y + block_noise_size_y] = window\n",
    "    local_shuffling_x = image_temp\n",
    "    # print('local_shuffling')\n",
    "    return local_shuffling_x\n",
    "\n",
    "\n",
    "def image_in_painting(x):\n",
    "    img_deps, img_rows, img_cols = x.shape\n",
    "    cnt = 5\n",
    "    while cnt > 0 and random.random() < 0.95:\n",
    "        block_noise_size_x = random.randint(img_rows // 20, img_rows // 10)\n",
    "        block_noise_size_y = random.randint(img_cols // 20, img_cols // 10)\n",
    "        # block_noise_size_z = random.randint(img_deps//6, img_deps//3)\n",
    "        noise_x = random.randint(3, img_rows - block_noise_size_x - 3)\n",
    "        noise_y = random.randint(3, img_cols - block_noise_size_y - 3)\n",
    "        # noise_z = random.randint(3, img_deps-block_noise_size_z-3)\n",
    "\n",
    "        x[0,\n",
    "        noise_x:noise_x + block_noise_size_x,\n",
    "        noise_y:noise_y + block_noise_size_y] = np.full((block_noise_size_x, block_noise_size_y),\n",
    "                                                        np.random.rand(1)[0] * 255)\n",
    "    # print('inpaint')    \n",
    "    return x\n",
    "\n",
    "\n",
    "def image_out_painting(x):\n",
    "    img_deps, img_rows, img_cols, = x.shape\n",
    "    image_temp = copy.deepcopy(x)\n",
    "    x = np.full((x.shape[0], x.shape[1], x.shape[2]), np.random.rand(1)[0] * 255)\n",
    "    block_noise_size_x = img_rows - random.randint(3 * img_rows // 12, 4 * img_rows // 12)\n",
    "    block_noise_size_y = img_cols - random.randint(3 * img_cols // 12, 4 * img_cols // 12)\n",
    "    noise_x = random.randint(3, img_rows - block_noise_size_x - 3)\n",
    "    noise_y = random.randint(3, img_cols - block_noise_size_y - 3)\n",
    "    x[0,\n",
    "    noise_x:noise_x + block_noise_size_x,\n",
    "    noise_y:noise_y + block_noise_size_y, ] = image_temp[0, noise_x:noise_x + block_noise_size_x,\n",
    "                                              noise_y:noise_y + block_noise_size_y]\n",
    "    cnt = 4\n",
    "    while cnt > 0 and random.random() < 0.95:\n",
    "        block_noise_size_x = img_rows - random.randint(3 * img_rows // 12, 4 * img_rows // 12)\n",
    "        block_noise_size_y = img_cols - random.randint(3 * img_cols // 12, 4 * img_cols // 12)\n",
    "        # block_noise_size_z = img_deps - random.randint(3*img_deps//7, 4*img_deps//7)\n",
    "        noise_x = random.randint(3, img_rows - block_noise_size_x - 3)\n",
    "        noise_y = random.randint(3, img_cols - block_noise_size_y - 3)\n",
    "        # noise_z = random.randint(3, img_deps-block_noise_size_z-3)\n",
    "        x[:,\n",
    "        noise_x:noise_x + block_noise_size_x,\n",
    "        noise_y:noise_y + block_noise_size_y] = image_temp[:, noise_x:noise_x + block_noise_size_x,\n",
    "                                                noise_y:noise_y + block_noise_size_y]\n",
    "    # print('out_paint')\n",
    "\n",
    "    return x\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "06idvN1Zdm5y"
   },
   "source": [
    "## Dataloader and Self-supervised dataset"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "9pM25RhU_on8"
   },
   "source": [
    "class SelfSupervisedDataset(Dataset):\n",
    "    \"\"\"Dementia Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, original_dataset_dir, preprocessed_dataset_dir, labels_file, begin_at_slice=6, num_slices=7,\n",
    "                 image_crop_size=128,\n",
    "                 label_type='PVWM', preprocess=True,\n",
    "                 transform=None):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            param original_dataset_dir (string): Path to the directory with all the original images.\n",
    "            param preprocessed_dataset_dir (string): Path to the directory with all the preprocessed images,\n",
    "            param labels_file (string): Path to the csv file with PVWM and DWM labels.\n",
    "            begin_at_slice (int): The first slice number to use. This is because we are interested in middle slices.\n",
    "            num_slices (int): Number of slices from each MRI series to use.\n",
    "            image_crop_size (int): The height and width of a center patch extracted from each original image.\n",
    "            label_type (string): Assumes values 'PVWM', 'DWM'.\n",
    "            preprocess (boolean): Whether to preprocess samples.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "\n",
    "        assert label_type in ['PVWM', 'DWM'], \"Invalid label type {}, label type must be one of ['PVWM', 'DWM']\".format(\n",
    "            label_type)\n",
    "\n",
    "        self.original_dataset_dir = original_dataset_dir\n",
    "        self.preprocessed_dataset_dir = preprocessed_dataset_dir\n",
    "        self.labels = pd.read_csv(labels_file, dtype={\"ID\": str})\n",
    "        self.begin_at_slice = begin_at_slice\n",
    "        self.num_slices = num_slices\n",
    "        self.image_size = image_crop_size\n",
    "        self.label_type = label_type\n",
    "        self.preprocess = preprocess\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "\n",
    "        slices = self.get_original_slices(idx)\n",
    "\n",
    "        if self.preprocess:\n",
    "            preprocessed_slices = self.get_preprocessed_slices(idx)\n",
    "            slices['slices'] = torch.cat([slices['slices'], preprocessed_slices['slices']], dim=0)\n",
    "            slices['label'] = torch.cat([slices['label'], preprocessed_slices['label']], dim=0)\n",
    "        sample = [slices['slices'], slices['label']]\n",
    "        return sample\n",
    "\n",
    "    def get_original_slices(self, idx):\n",
    "\n",
    "        slices = torch.zeros((self.num_slices, 1, self.image_size, self.image_size))\n",
    "        slices_label = torch.zeros((self.num_slices, 1, self.image_size, self.image_size))\n",
    "\n",
    "        patient_id = self.labels.loc[idx, 'ID'] # patient id in the dataset\n",
    "        patient_dir = os.path.join(self.original_dataset_dir, patient_id)\n",
    "\n",
    "        all_slices = sorted(os.listdir(patient_dir))\n",
    "        middle_slices = all_slices[self.begin_at_slice: self.begin_at_slice + self.num_slices]\n",
    "\n",
    "        for i, slice_file in enumerate(middle_slices):\n",
    "            slice_file_path = os.path.join(patient_dir, slice_file)\n",
    "            slice_data = pydicom.dcmread(slice_file_path).pixel_array.astype(float)\n",
    "            slice_data *= 255.0 / np.max(slice_data)  # normalize all images to [0-255]\n",
    "            slice_data = slice_data.astype(np.uint8)\n",
    "\n",
    "            if self.transform:\n",
    "                slice_data = (self.transform(np.expand_dims(slice_data, 2))).numpy() * 255\n",
    "            x = copy.deepcopy(slice_data)\n",
    "            x, slice_data = data_augmentation(x, slice_data, config.flip_rate)\n",
    "            x = local_pixel_shuffling(x, prob=config.local_rate)\n",
    "            x = nonlinear_transformation(x, config.nonlinear_rate)\n",
    "            if random.random() < config.paint_rate:\n",
    "                if random.random() < config.inpaint_rate:\n",
    "                    x = image_in_painting(x)\n",
    "                else:\n",
    "                    x = image_out_painting(x)\n",
    "\n",
    "            x_train_tensor = torch.from_numpy(np.true_divide(x, 255)).unsqueeze(0)\n",
    "            x_label_tensor = torch.from_numpy(np.true_divide(slice_data, 255)).unsqueeze(0)\n",
    "\n",
    "            slices[i] = x_train_tensor\n",
    "            slices_label[i] = x_label_tensor\n",
    "        sample = {'slices': slices, 'label': slices_label}\n",
    "        return sample\n",
    "\n",
    "    def get_preprocessed_slices(self, idx):\n",
    "        slices = torch.zeros((self.num_slices, 1, self.image_size, self.image_size))\n",
    "        slices_label = torch.zeros((self.num_slices, 1, self.image_size, self.image_size))\n",
    "\n",
    "        patient_id = self.labels.loc[idx, 'ID'] # patient id in the dataset\n",
    "        patient_dir = os.path.join(self.original_dataset_dir, patient_id)\n",
    "\n",
    "        all_slices = sorted(os.listdir(patient_dir))\n",
    "        middle_slices = all_slices[self.begin_at_slice: self.begin_at_slice + self.num_slices]\n",
    "\n",
    "        for i, slice_file in enumerate(middle_slices):\n",
    "            slice_file_path = os.path.join(patient_dir, slice_file)\n",
    "\n",
    "            slice_data = io.imread(slice_file_path)\n",
    "            slice_data = (slice_data * (255.0 / np.max(slice_data)))  # normalize all images to [0-255]\n",
    "            slice_data = slice_data.astype(np.uint8)\n",
    "\n",
    "            if self.transform:\n",
    "                slice_data = (self.transform(np.expand_dims(slice_data, 2))).numpy() * 255\n",
    "\n",
    "            x = copy.deepcopy(slice_data)\n",
    "            x, slice_data = data_augmentation(x, slice_data, config.flip_rate)\n",
    "            x = local_pixel_shuffling(x, prob=config.local_rate)\n",
    "            x = nonlinear_transformation(x, config.nonlinear_rate)\n",
    "            if random.random() < config.paint_rate:\n",
    "                if random.random() < config.inpaint_rate:\n",
    "                    x = image_in_painting(x)\n",
    "                else:\n",
    "                    x = image_out_painting(x)\n",
    "\n",
    "            x_train_tensor = torch.from_numpy(np.true_divide(x, 255)).unsqueeze(0)\n",
    "            x_label_tensor = torch.from_numpy(np.true_divide(slice_data, 255)).unsqueeze(0)\n",
    "\n",
    "            slices[i] = x_train_tensor\n",
    "            slices_label[i] = x_label_tensor\n",
    "        sample = {'slices': slices, 'label': slices_label}\n",
    "        return sample"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "42cvem-XkAmD"
   },
   "source": [
    "transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                transforms.RandomRotation(degrees=45),\n",
    "                                transforms.CenterCrop(128),\n",
    "                                transforms.ToTensor()])\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "qtK3Iy9DAwmO"
   },
   "source": [
    "dataset = SelfSupervisedDataset(original_dataset_dir=config.original_dataset_dir,\n",
    "                                preprocessed_dataset_dir=config.preprocessed_dataset_dir,\n",
    "                                labels_file=config.labels_file,\n",
    "                                label_type=config.label_type,\n",
    "                                preprocess=config.preprocess,\n",
    "                                begin_at_slice=config.begin_at_slice,\n",
    "                                num_slices=config.num_slices,\n",
    "                                image_crop_size=config.image_crop_size,\n",
    "                                transform=transform\n",
    "                                )"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "w3gWqCtc-Qvy",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "6b0c0aa5-3ab8-41f6-c725-a3491ddea732"
   },
   "source": [
    "train_split = 0.9\n",
    "batch_size = 4\n",
    "\n",
    "dataset_size = len(dataset)\n",
    "train_size = int(np.floor(train_split * dataset_size))\n",
    "validation_size = dataset_size - train_size\n",
    "\n",
    "train_set, val_set = torch.utils.data.random_split(dataset, [train_size, validation_size])\n",
    "\n",
    "train_samples_weights = torch.from_numpy(np.true_divide(np.ones(len(train_set)), len(train_set)))\n",
    "print(train_samples_weights)\n",
    "train_sampler = WeightedRandomSampler(weights=train_samples_weights, num_samples=len(train_samples_weights),\n",
    "                                      replacement=True)\n",
    "\n",
    "dataloaders = {\n",
    "    'train': DataLoader(train_set, batch_size=batch_size, sampler=train_sampler),\n",
    "    'val': DataLoader(val_set, batch_size=batch_size, )\n",
    "}\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Sava data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images, labels = next(iter(dataloaders[\"train\"]))"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "os.mkdir('../log_dir/ss/check2')\n",
    "os.mkdir('../log_dir/ss/check2/image')\n",
    "os.mkdir('../log_dir/ss/check2/label')\n",
    "num = 0\n",
    "for i in range(images.shape[0]):\n",
    "    for j in range(images.shape[1]):\n",
    "        print(i,j)\n",
    "        image.imsave(os.path.join('../log_dir/ss/check2/image/' ,'{}.png'.format(num)), images[i][j][0], cmap='gray')\n",
    "        image.imsave(os.path.join('../log_dir/ss/check2/label/' ,'{}.png'.format(num)), labels[i][j][0], cmap='gray')\n",
    "        num += 1"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "images.shape"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "labels.shape\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XLvcs7NIdWWX"
   },
   "source": [
    "# Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fn0JDn5Qe30m"
   },
   "source": [
    "## Resnet18"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1JcNilJ3lP7x"
   },
   "source": [
    "__all__ = ['ResNet', 'resnet18']\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.hub import load_state_dict_from_url\n",
    "from torch.nn import init\n",
    "\n",
    "model_urls = {\n",
    "    'resnet18': 'https://download.pytorch.org/models/resnet18-5c106cde.pth',\n",
    "    'resnet34': 'https://download.pytorch.org/models/resnet34-333f7ec4.pth',\n",
    "    'resnet50': 'https://download.pytorch.org/models/resnet50-19c8e357.pth',\n",
    "    'resnet101': 'https://download.pytorch.org/models/resnet101-5d3b4d8f.pth',\n",
    "    'resnet152': 'https://download.pytorch.org/models/resnet152-b121ed2d.pth',\n",
    "    'resnext50_32x4d': 'https://download.pytorch.org/models/resnext50_32x4d-7cdf4587.pth',\n",
    "    'resnext101_32x8d': 'https://download.pytorch.org/models/resnext101_32x8d-8ba56ff5.pth',\n",
    "    'wide_resnet50_2': 'https://download.pytorch.org/models/wide_resnet50_2-95faca4d.pth',\n",
    "    'wide_resnet101_2': 'https://download.pytorch.org/models/wide_resnet101_2-32ee1156.pth',\n",
    "}\n",
    "\n",
    "\n",
    "def conv3x3(in_planes, out_planes, stride=1, groups=1, dilation=1):\n",
    "    \"\"\"3x3 convolution with padding\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                     padding=dilation, groups=groups, bias=False, dilation=dilation)\n",
    "\n",
    "\n",
    "def conv1x1(in_planes, out_planes, stride=1):\n",
    "    \"\"\"1x1 convolution\"\"\"\n",
    "    return nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride, bias=False)\n",
    "\n",
    "\n",
    "class BasicBlock(nn.Module):\n",
    "    expansion = 1\n",
    "    __constants__ = ['downsample']\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=2, dilation=1, norm_layer=None):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        if groups != 1 or base_width != 2:\n",
    "            raise ValueError('BasicBlock only supports groups=1 and base_width=64')\n",
    "        if dilation > 1:\n",
    "            raise NotImplementedError(\"Dilation > 1 not supported in BasicBlock\")\n",
    "        # Both self.conv1 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv3x3(inplanes, planes, stride)\n",
    "        self.bn1 = norm_layer(planes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.conv2 = conv3x3(planes, planes)\n",
    "        self.bn2 = norm_layer(planes)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class Bottleneck(nn.Module):\n",
    "    expansion = 4\n",
    "    __constants__ = ['downsample']\n",
    "\n",
    "    def __init__(self, inplanes, planes, stride=1, downsample=None, groups=1,\n",
    "                 base_width=2, dilation=1, norm_layer=None):\n",
    "        super(Bottleneck, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        width = int(planes * (base_width / 2.)) * groups\n",
    "        # Both self.conv2 and self.downsample layers downsample the input when stride != 1\n",
    "        self.conv1 = conv1x1(inplanes, width)\n",
    "        self.bn1 = norm_layer(width)\n",
    "        self.conv2 = conv3x3(width, width, stride, groups, dilation)\n",
    "        self.bn2 = norm_layer(width)\n",
    "        self.conv3 = conv1x1(width, planes * self.expansion)\n",
    "        self.bn3 = norm_layer(planes * self.expansion)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.downsample = downsample\n",
    "        self.stride = stride\n",
    "\n",
    "    def forward(self, x):\n",
    "        identity = x\n",
    "\n",
    "        out = self.conv1(x)\n",
    "        out = self.bn1(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv2(out)\n",
    "        out = self.bn2(out)\n",
    "        out = self.relu(out)\n",
    "\n",
    "        out = self.conv3(out)\n",
    "        out = self.bn3(out)\n",
    "\n",
    "        if self.downsample is not None:\n",
    "            identity = self.downsample(x)\n",
    "\n",
    "        out += identity\n",
    "        out = self.relu(out)\n",
    "\n",
    "        return out\n",
    "\n",
    "\n",
    "class ResNet(nn.Module):\n",
    "\n",
    "    def __init__(self, block, layers, num_classes=1000, zero_init_residual=False,\n",
    "                 groups=1, width_per_group=2, replace_stride_with_dilation=None,\n",
    "                 norm_layer=None):\n",
    "        super(ResNet, self).__init__()\n",
    "        if norm_layer is None:\n",
    "            norm_layer = nn.BatchNorm2d\n",
    "        self._norm_layer = norm_layer\n",
    "\n",
    "        self.inplanes = 2\n",
    "        self.dilation = 1\n",
    "        if replace_stride_with_dilation is None:\n",
    "            # each element in the tuple indicates if we should replace\n",
    "            # the 2x2 stride with a dilated convolution instead\n",
    "            replace_stride_with_dilation = [False, False, False]\n",
    "        if len(replace_stride_with_dilation) != 3:\n",
    "            raise ValueError(\"replace_stride_with_dilation should be None \"\n",
    "                             \"or a 3-element tuple, got {}\".format(replace_stride_with_dilation))\n",
    "        self.groups = groups\n",
    "        self.base_width = width_per_group\n",
    "        self.conv1 = nn.Conv2d(1, self.inplanes, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.bn1 = norm_layer(self.inplanes)\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.maxpool = nn.MaxPool2d(kernel_size=3, stride=2, padding=1)\n",
    "        self.layer1 = self._make_layer(block, 2, layers[0])\n",
    "        self.layer2 = self._make_layer(block, 4, layers[1], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[0])\n",
    "        self.layer3 = self._make_layer(block, 8, layers[2], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[1])\n",
    "        self.layer4 = self._make_layer(block, 16, layers[3], stride=2,\n",
    "                                       dilate=replace_stride_with_dilation[2])\n",
    "        self.avgpool = nn.AdaptiveAvgPool2d((1, 1))\n",
    "        self.fc = nn.Linear(16 * block.expansion, num_classes)\n",
    "\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, (nn.BatchNorm2d, nn.GroupNorm)):\n",
    "                nn.init.constant_(m.weight, 1)\n",
    "                nn.init.constant_(m.bias, 0)\n",
    "\n",
    "        # Zero-initialize the last BN in each residual branch,\n",
    "        # so that the residual branch starts with zeros, and each residual block behaves like an identity.\n",
    "        # This improves the model by 0.2~0.3% according to https://arxiv.org/abs/1706.02677\n",
    "        if zero_init_residual:\n",
    "            for m in self.modules():\n",
    "                if isinstance(m, Bottleneck):\n",
    "                    nn.init.constant_(m.bn3.weight, 0)\n",
    "                elif isinstance(m, BasicBlock):\n",
    "                    nn.init.constant_(m.bn2.weight, 0)\n",
    "\n",
    "    def _make_layer(self, block, planes, blocks, stride=1, dilate=False):\n",
    "        norm_layer = self._norm_layer\n",
    "        downsample = None\n",
    "        previous_dilation = self.dilation\n",
    "        if dilate:\n",
    "            self.dilation *= stride\n",
    "            stride = 1\n",
    "        if stride != 1 or self.inplanes != planes * block.expansion:\n",
    "            downsample = nn.Sequential(\n",
    "                conv1x1(self.inplanes, planes * block.expansion, stride),\n",
    "                norm_layer(planes * block.expansion),\n",
    "            )\n",
    "\n",
    "        layers = [block(self.inplanes, planes, stride, downsample, self.groups,\n",
    "                        self.base_width, previous_dilation, norm_layer)]\n",
    "        self.inplanes = planes * block.expansion\n",
    "        for _ in range(1, blocks):\n",
    "            layers.append(block(self.inplanes, planes, groups=self.groups,\n",
    "                                base_width=self.base_width, dilation=self.dilation,\n",
    "                                norm_layer=norm_layer))\n",
    "\n",
    "        return nn.Sequential(*layers)\n",
    "\n",
    "    def _forward_impl(self, x):\n",
    "        # See note [TorchScript super()]r======\n",
    "        x = self.conv1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.relu(x)\n",
    "        x = self.maxpool(x)\n",
    "\n",
    "        x = self.layer1(x)\n",
    "        x = self.layer2(x)\n",
    "        x = self.layer3(x)\n",
    "        x = self.layer4(x)\n",
    "\n",
    "        x = self.avgpool(x)\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self._forward_impl(x)\n",
    "\n",
    "\n",
    "def _resnet(arch, block, layers, pretrained, progress, **kwargs):\n",
    "    model = ResNet(block, layers, **kwargs)\n",
    "    if pretrained:\n",
    "        state_dict = load_state_dict_from_url(model_urls[arch],\n",
    "                                              progress=progress)\n",
    "        model.load_state_dict(state_dict)\n",
    "    return model\n",
    "\n",
    "\n",
    "def resnet18(pretrained=False, progress=True, **kwargs):\n",
    "    \"\"\"ResNet-18 model from\n",
    "    `\"Deep Residual Learning for Image Recognition\" <https://arxiv.org/pdf/1512.03385.pdf>`_\n",
    "    Args:\n",
    "        pretrained (bool): If True, returns a model pre-trained on ImageNet\n",
    "        progress (bool): If True, displays a progress bar of the download to stderr\n",
    "    \"\"\"\n",
    "    return _resnet('resnet18', BasicBlock, [2, 2, 2, 2], pretrained, progress,\n",
    "                   **kwargs)\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pTAA3kvtlmEm",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "ae3ba3a9-3cc1-4e16-f0d3-f409c0387678"
   },
   "source": [
    "base_model = resnet18(pretrained=False)\n",
    "\n",
    "print(list(base_model.children()))"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D-sS3KEYe93d"
   },
   "source": [
    "## ResnetUnet"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "moyhuaixCHg8"
   },
   "source": [
    "\n",
    "def init_weights(m):\n",
    "    if type(m) == nn.Linear:\n",
    "        init.kaiming_normal_(m.weight, mode='fan_in')\n",
    "        m.bias.data.fill_(0.01)\n",
    "\n",
    "\n",
    "def convrelu(in_channels, out_channels, kernel, padding):\n",
    "    return nn.Sequential(\n",
    "        nn.Conv2d(in_channels, out_channels, kernel, padding=padding),\n",
    "        nn.ReLU(inplace=True),\n",
    "    )\n",
    "\n",
    "\n",
    "class ResNetUNet(nn.Module):\n",
    "\n",
    "    def __init__(self, n_class):\n",
    "        super(ResNetUNet, self).__init__()\n",
    "\n",
    "        self.base_model = resnet18(pretrained=False)\n",
    "\n",
    "        self.base_layers = list(self.base_model.children())\n",
    "        self.layer0 = nn.Sequential(*self.base_layers[0:3])  # size=(N, 64, x.H/2, x.W/2)\n",
    "        self.layer0.apply(init_weights)\n",
    "\n",
    "        self.layer0_1x1 = convrelu(2, 2, 1, 0)\n",
    "        self.layer0_1x1.apply(init_weights)\n",
    "        self.layer1 = nn.Sequential(*self.base_layers[3:5])  # size=(N, 64, x.H/4, x.W/4)\n",
    "        self.layer1.apply(init_weights)\n",
    "\n",
    "        self.layer1_1x1 = convrelu(2, 2, 1, 0)\n",
    "        self.layer1_1x1.apply(init_weights)\n",
    "        self.layer2 = self.base_layers[5]  # size=(N, 128, x.H/8, x.W/8)\n",
    "        self.layer2.apply(init_weights)\n",
    "\n",
    "        self.layer2_1x1 = convrelu(4, 4, 1, 0)\n",
    "        self.layer2_1x1.apply(init_weights)\n",
    "        self.layer3 = self.base_layers[6]  # size=(N, 256, x.H/16, x.W/16)\n",
    "        self.layer3.apply(init_weights)\n",
    "\n",
    "        self.layer3_1x1 = convrelu(8, 8, 1, 0)\n",
    "        self.layer3_1x1.apply(init_weights)\n",
    "        self.layer4 = self.base_layers[7]  # size=(N, 512, x.H/32, x.W/32)\n",
    "        self.layer4.apply(init_weights)\n",
    "\n",
    "        self.layer4_1x1 = convrelu(16, 16, 1, 0)\n",
    "        self.layer4_1x1.apply(init_weights)\n",
    "\n",
    "        self.upsample = nn.Upsample(scale_factor=2, mode='bilinear', align_corners=True)\n",
    "\n",
    "        self.conv_up3 = convrelu(8 + 16, 16, 3, 1)\n",
    "        self.conv_up2 = convrelu(4 + 16, 8, 3, 1)\n",
    "        self.conv_up1 = convrelu(2 + 8, 8, 3, 1)\n",
    "        self.conv_up0 = convrelu(2 + 8, 4, 3, 1)\n",
    "\n",
    "        self.conv_up3.apply(init_weights)\n",
    "        self.conv_up2.apply(init_weights)\n",
    "        self.conv_up1.apply(init_weights)\n",
    "        self.conv_up0.apply(init_weights)\n",
    "\n",
    "        self.conv_original_size0 = convrelu(1, 2, 3, 1)\n",
    "        self.conv_original_size1 = convrelu(2, 2, 3, 1)\n",
    "        self.conv_original_size2 = convrelu(2 + 4, 2, 3, 1)\n",
    "\n",
    "        self.conv_original_size0.apply(init_weights)\n",
    "        self.conv_original_size1.apply(init_weights)\n",
    "        self.conv_original_size2.apply(init_weights)\n",
    "\n",
    "        self.conv_last = nn.Conv2d(2, n_class, 1)\n",
    "        init.kaiming_normal_(self.conv_last.weight, mode='fan_in')\n",
    "\n",
    "    def forward(self, x, decode=True):\n",
    "        x_original = self.conv_original_size0(x)\n",
    "        x_original = self.conv_original_size1(x_original)\n",
    "\n",
    "        layer0, layer1, layer2, layer3, layer4 = self.encode(x)\n",
    "\n",
    "        if decode:\n",
    "            out = self.decode(x_original, layer0, layer1, layer2, layer3, layer4)\n",
    "            return out\n",
    "\n",
    "        return layer4\n",
    "\n",
    "    def encode(self, x):\n",
    "        layer0 = self.layer0(x)\n",
    "        layer1 = self.layer1(layer0)\n",
    "        layer2 = self.layer2(layer1)\n",
    "        layer3 = self.layer3(layer2)\n",
    "        layer4 = self.layer4(layer3)\n",
    "\n",
    "        layer4 = self.layer4_1x1(layer4)\n",
    "        return layer0, layer1, layer2, layer3, layer4\n",
    "\n",
    "    def decode(self, x_original, layer0, layer1, layer2, layer3, layer4):\n",
    "        x = self.upsample(layer4)\n",
    "        layer3 = self.layer3_1x1(layer3)\n",
    "        x = torch.cat([x, layer3], dim=1)\n",
    "        x = self.conv_up3(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer2 = self.layer2_1x1(layer2)\n",
    "        x = torch.cat([x, layer2], dim=1)\n",
    "        x = self.conv_up2(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer1 = self.layer1_1x1(layer1)\n",
    "        x = torch.cat([x, layer1], dim=1)\n",
    "        x = self.conv_up1(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        layer0 = self.layer0_1x1(layer0)\n",
    "        x = torch.cat([x, layer0], dim=1)\n",
    "        x = self.conv_up0(x)\n",
    "\n",
    "        x = self.upsample(x)\n",
    "        x = torch.cat([x, x_original], dim=1)\n",
    "        x = self.conv_original_size2(x)\n",
    "\n",
    "        out = self.conv_last(x)\n",
    "\n",
    "        return out\n"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "2wl9o2NB4XYs",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "e60d57d8-27f8-4119-eb06-3197d4bb0054"
   },
   "source": [
    "base_model = ResNetUNet(1)\n",
    "\n",
    "list(base_model.children())"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Wkv-5f-MfDd-"
   },
   "source": [
    "# Train"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "msKur7_KC95t"
   },
   "source": [
    "def save_checkpoint(state,\n",
    "                    filename='../models/ss_models/self_supervised_model.pth.tar'):\n",
    "    torch.save(state, filename)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SWZMBuoQJ97G"
   },
   "source": [
    "\n",
    "\n",
    "def calc_loss(pred, target, metrics, bce_weight=0.5):\n",
    "    # bce = F.binary_cross_entropy_with_logits(pred, target)\n",
    "\n",
    "    # pred = torch.sigmoid(pred)\n",
    "    # dice = dice_loss(pred, target)\n",
    "\n",
    "    # loss = bce * bce_weight + dice * (1 - bce_weight)\n",
    "\n",
    "    # metrics['bce'] += bce.data.cpu().numpy() * target.size(0)\n",
    "    # metrics['dice'] += dice.data.cpu().numpy() * target.size(0)\n",
    "\n",
    "    loss = nn.MSELoss()\n",
    "    output = loss(pred, target)\n",
    "    # print(output)\n",
    "    metrics['loss'] += output.data.cpu().numpy() * target.size(0)\n",
    "    # return loss\n",
    "    return output\n",
    "\n",
    "\n",
    "def print_metrics(metrics, epoch_samples, phase):\n",
    "    outputs = []\n",
    "    for k in metrics.keys():\n",
    "        outputs.append(\"{}: {:4f}\".format(k, metrics[k] / epoch_samples))\n",
    "\n",
    "    print(\"{}: {}\".format(phase, \", \".join(outputs)))\n",
    "\n",
    "\n",
    "def train_model(model, optimizer, scheduler, num_epochs=25):\n",
    "    best_model_wts = copy.deepcopy(model.state_dict())\n",
    "    best_loss = 1e10\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "\n",
    "        since = time.time()\n",
    "        # uuu = 0\n",
    "        # Each epoch has a training and validation phase\n",
    "        for phase in ['train', 'val']:\n",
    "            if phase == 'train':\n",
    "                for param_group in optimizer.param_groups:\n",
    "                    print(\"LR\", param_group['lr'])\n",
    "\n",
    "                model.train()  # Set model to training mode\n",
    "            else:\n",
    "                model.eval()  # Set model to evaluate mode\n",
    "            metrics = defaultdict(float)\n",
    "            epoch_samples = 0\n",
    "\n",
    "            for inputs, labels in dataloaders[phase]:\n",
    "                inputs = ((inputs.reshape(-1, 1, 128, 128)).float()).to(device)\n",
    "                labels = ((labels.reshape(-1, 1, 128, 128)).float()).to(device)\n",
    "                optimizer.zero_grad()\n",
    "\n",
    "                # forward\n",
    "                # track history if only in train\n",
    "                with torch.set_grad_enabled(phase == 'train'):\n",
    "                    print(\"inputs.shape\", inputs.shape)\n",
    "                    outputs = model(inputs.float())\n",
    "                    loss = calc_loss(outputs, labels, metrics)\n",
    "\n",
    "                    # backward + optimize only if in training phase\n",
    "                    if phase == 'train':\n",
    "                        loss.backward()\n",
    "                        optimizer.step()\n",
    "\n",
    "                # statistics\n",
    "                epoch_samples += inputs.size(0)\n",
    "\n",
    "            print_metrics(metrics, epoch_samples, phase)\n",
    "            epoch_loss = metrics['loss'] / epoch_samples\n",
    "            scheduler.step(epoch_loss)\n",
    "\n",
    "            # deep copy the model\n",
    "            if phase == 'val' and epoch_loss < best_loss:\n",
    "                print(\"saving best model\")\n",
    "                best_loss = epoch_loss\n",
    "                best_model_wts = copy.deepcopy(model.state_dict())\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'best_score': epoch_loss,\n",
    "                    'optimizer': optimizer.state_dict(),\n",
    "                })\n",
    "\n",
    "        time_elapsed = time.time() - since\n",
    "        print('{:.0f}m {:.0f}s'.format(time_elapsed // 60, time_elapsed % 60))\n",
    "\n",
    "    print('Best val loss: {:4f}'.format(best_loss))\n",
    "\n",
    "    # load best model weights\n",
    "    model.load_state_dict(best_model_wts)\n",
    "    return model"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "6g3yBjAvJ-3l",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "outputId": "ed3afd43-b67e-4aa0-a54a-c86eaef168ac"
   },
   "source": [
    "\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n",
    "\n",
    "num_class = 1\n",
    "\n",
    "model = ResNetUNet(n_class=1).to(device)\n",
    "model.float()\n",
    "\n",
    "# freeze backbone layers\n",
    "# Comment out to finetune further\n",
    "# for l in model.base_layers:\n",
    "#     for param in l.parameters():\n",
    "#         param.requires_grad = False\n",
    "\n",
    "optimizer_ft = optim.Adam(filter(lambda p: p.requires_grad, model.parameters()), lr=1e-3)\n",
    "\n",
    "lr_scheduler = ReduceLROnPlateau(optimizer_ft, \"min\", factor=0.95, patience=30, verbose=False)\n",
    "\n",
    "model = train_model(model, optimizer_ft, lr_scheduler, num_epochs=1000)"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "r0IFvd5jfJOy"
   },
   "source": [
    "#Test"
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "cNJutEK0KPUi",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "240c1b29-8480-42d6-f355-caa7b8fc4fd0"
   },
   "source": [
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(config.load_saved_model_path)\n",
    "checkpoint = torch.load(config.load_saved_model_path, map_location=device)\n",
    "resnet_unet = ResNetUNet(n_class=1).to(device)\n",
    "resnet_unet.load_state_dict(checkpoint['state_dict'])\n",
    "print(torch.load(config.load_saved_model_path, map_location=device)['epoch'])\n",
    "print(torch.load(config.load_saved_model_path, map_location=device)['best_score'])"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ctANk0fzLU5T"
   },
   "source": [
    "import shutil\n",
    "\n",
    "try:\n",
    "    shutil.rmtree('../log_dir/check')\n",
    "except FileNotFoundError:\n",
    "    pass"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "y3FvojdHKZFP",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "outputId": "850a624d-332c-45f9-8b7c-e4efb0f95d54"
   },
   "source": [
    "model.eval()\n",
    "test_loader = DataLoader(val_set, batch_size=2, )\n",
    "\n",
    "inputs, labels = next(iter(test_loader))\n",
    "os.mkdir('../log_dir/check')\n",
    "os.mkdir('../log_dir/check/image')\n",
    "os.mkdir('../log_dir/check/label')\n",
    "os.mkdir('../log_dir/check/reconstruction')\n",
    "num = 0\n",
    "for i in range(inputs.shape[0]):\n",
    "    for j in range(inputs.shape[1]):\n",
    "        print(i, j)\n",
    "        image.imsave(os.path.join('../log_dir/check/image/', '{}.png'.format(num)), inputs[i][j][0], cmap='gray')\n",
    "        image.imsave(os.path.join('../log_dir/check/label/', '{}.png'.format(num)), labels[i][j][0], cmap='gray')\n",
    "        num += 1\n",
    "inputs = ((inputs.reshape(-1, 1, 128, 128)).float()).to(device)\n",
    "labels = ((labels.reshape(-1, 1, 128, 128)).float()).to(device)\n",
    "pred = model(inputs)\n",
    "pred = torch.sigmoid(pred)\n",
    "pred = pred.data.cpu().numpy()\n",
    "print(pred.shape)\n",
    "for i in range(len(pred)):\n",
    "    image.imsave(os.path.join('../log_dir/check/reconstruction', '{}.png'.format(i)), pred[i][0], cmap='gray')"
   ],
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}