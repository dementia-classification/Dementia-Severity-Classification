{
 "nbformat": 4,
 "nbformat_minor": 0,
 "metadata": {
  "colab": {
   "name": "KFCV WM Lesion Prediction.ipynb",
   "provenance": [
    {
     "file_id": "1UNbfutsIZR09N3Awfezh2cVDAphaOhQo",
     "timestamp": 1588138254780
    }
   ],
   "collapsed_sections": [],
   "machine_shape": "hm"
  },
  "kernelspec": {
   "name": "python3",
   "language": "python",
   "display_name": "Python 3"
  },
  "accelerator": "GPU"
 },
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "%load_ext tensorboard"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import os\n",
    "from skimage import io\n",
    "from collections import OrderedDict\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from self_supervised import ResNetUNet\n",
    "\n",
    "from sklearn.metrics import accuracy_score, f1_score\n",
    "\n",
    "from copy import copy\n",
    "\n",
    "import pydicom\n",
    "\n",
    "import torch\n",
    "import torchvision\n",
    "from torch.utils.data import Dataset, DataLoader, Subset, WeightedRandomSampler\n",
    "from torchvision import transforms\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.optim.lr_scheduler import ReduceLROnPlateau"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "for directory in [\"../models/\", \"../log_dir/\"]:\n",
    "    try:\n",
    "        os.mkdir(directory)\n",
    "    except FileExistsError:\n",
    "        pass"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "hKP8usivWP60"
   },
   "source": [
    "class Config:\n",
    "\n",
    "    def __init__(self,\n",
    "                 original_dataset_dir='../data/Dataset/',\n",
    "                 preprocessed_dataset_dir='../data/Preprocessed_Dataset/',\n",
    "                 labels_file='../data/labels.csv',\n",
    "                 dataset_begin_at_slice=6,\n",
    "                 dataset_num_slice=7,\n",
    "                 image_crop_size=128,\n",
    "                 label_type='PVWM',\n",
    "                 n_classes=4,\n",
    "                 preprocess=True,\n",
    "                 self_supervised=True,\n",
    "                 xavier_initialization=False,\n",
    "                 self_supervised_model_path='../models/ss_models/feature_less_4_transformation_model3(batch2,rotation45).pth.tar',\n",
    "                 cuda=True,\n",
    "                 tensorboard_dir='../log_dir/runs',\n",
    "                 save_best_model=True,\n",
    "                 save_dir='../models',\n",
    "                 batch_size=1,\n",
    "                 lr = 0.008,\n",
    "                 epochs = 120,\n",
    "                 verbose_epoch = 20,\n",
    "                 regularization = 0.0,\n",
    "                 patience = 25,\n",
    "                 lr_decay = 0.8,\n",
    "                 n_folds=10,\n",
    "                 test_size=0.1,\n",
    "                 verbose=True):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            param original_dataset_dir (string): Path to the directory with all the original images.\n",
    "            param preprocessed_dataset_dir (string): Path to the directory with all the preprocessed images,\n",
    "            param labels_file (string): Path to the csv file with PVWM and DWM labels.\n",
    "            param dataset_begin_at_slice (int): The first slice number to use. This is because we are interested in middle slices.\n",
    "            param dataset_num_slices (int): Number of slices from each MRI serie to use.\n",
    "            param image_crop_size (int): The height and width of a center patch extracted from each original image.\n",
    "            param label_type (string): Assumes values 'PVWM' or 'DWM'.\n",
    "            param n_classes (int): Number of target classes of classification. Default is four as each biomarkers has four severity grades.\n",
    "            param preprocess (boolean): Whether to preprocess samples.\n",
    "            param self_supervised (boolean): Whether to include the self-supervised component. In fact whether to use self-supervised weights as a training initial point.\n",
    "            param xavier_initialization (boolean): Whether to initialze the network wights with xavier. Mutually exclusive with param \"self_supervised\".\n",
    "            param self_supervised_model_path (string): the path to the pretrianed self-supervised model.\n",
    "            param cuda (boolean): whether to train on gpu.\n",
    "            param tensorboard_dir (string): the path to the directory where tensorboard logs are to be stored.\n",
    "            param save_best_model (boolean): whether to save the best perfoming model.\n",
    "            param save_dir (string): if \"save_best_model\" is set to true, this param specifies the directory to save the best model.\n",
    "            param batch_size (int): Batch size of training and validation data ⚠️⚠️⚠️: Don't change this!\n",
    "            param lr (float): initial learning rate of the training process.\n",
    "            param epochs (int): maximum epochs to train the model in each cross-validation fold.\n",
    "            param verbose_epoch (int): the epoch step to print the train and validation loss and f1-score to console.\n",
    "            param regularization (float): the l1-regularization weight.\n",
    "            param patience (int): number of epochs with unchanged validatoin loss to wait. this is used as input to LRReduceOnPlateau instace.\n",
    "            param lr_decay (float): the amount of decay in learning rate after a plateau in validation loss.\n",
    "            param n_folds (int): number of CV folds.\n",
    "            param test_size (float): in no test set available this param specifies the portion of the training data as test data.\n",
    "            param verbose (boolean): used as input to LRReduceOnPlateau instance. to print the changes to learning rate to console.\n",
    "        \"\"\"\n",
    "\n",
    "        self.original_dataset_dir = original_dataset_dir\n",
    "        self.preprocessed_dataset_dir = preprocessed_dataset_dir\n",
    "        self.labels_file = labels_file\n",
    "        self.dataset_begin_at_slice = dataset_begin_at_slice\n",
    "        self.dataset_num_slice = dataset_num_slice\n",
    "        self.image_crop_size = image_crop_size\n",
    "        self.label_type = label_type\n",
    "        self.preprocess = preprocess\n",
    "        self.batch_size = batch_size\n",
    "        self.n_classes = n_classes\n",
    "        self.cuda = cuda\n",
    "\n",
    "        self.train_transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                      transforms.Grayscale(num_output_channels=1),\n",
    "                                      transforms.RandomHorizontalFlip(p=0.5),\n",
    "                                      transforms.RandomRotation(degrees=20),\n",
    "                                      transforms.CenterCrop(self.image_crop_size),\n",
    "                                      transforms.ToTensor()\n",
    "                                      ])\n",
    "\n",
    "        self.validation_transform = transforms.Compose([transforms.ToPILImage(),\n",
    "                                           transforms.Grayscale(num_output_channels=1),\n",
    "                                           transforms.CenterCrop(self.image_crop_size),\n",
    "                                           transforms.ToTensor()\n",
    "                                           ])\n",
    "\n",
    "        self.self_supervised = self_supervised\n",
    "        self.self_supervised_model_path = self_supervised_model_path\n",
    "        self.xavier_initialization = xavier_initialization\n",
    "\n",
    "        self.tensorboard_dir = tensorboard_dir\n",
    "        self.save_dir = save_dir\n",
    "        self.save_best_model = save_best_model\n",
    "\n",
    "        self.lr = lr\n",
    "        self.epochs = epochs\n",
    "        self.verbose_epoch = verbose_epoch\n",
    "        self.regularization = regularization\n",
    "        self.reduce_lr_patience = patience\n",
    "        self.lr_decay = lr_decay\n",
    "\n",
    "        self.n_folds = n_folds\n",
    "        self.test_size = test_size\n",
    "        self.verbose = verbose\n",
    "\n",
    "    def display(self):\n",
    "        \"\"\"Display Configuration values.\"\"\"\n",
    "        print(\"\\nConfigurations:\")\n",
    "        for a in dir(self):\n",
    "            if not a.startswith(\"__\") and not callable(getattr(self, a)):\n",
    "                print(\"{:30} {}\".format(a, getattr(self, a)))\n",
    "        print(\"\\n\")\n",
    "\n",
    "\n",
    "config = Config()\n",
    "config.display()"
   ],
   "execution_count": 3,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Configurations:\n",
      "batch_size                     1\n",
      "cuda                           False\n",
      "dataset_begin_at_slice         6\n",
      "dataset_num_slice              7\n",
      "epochs                         120\n",
      "image_crop_size                128\n",
      "label_type                     PVWM\n",
      "labels_file                    ../data/labels.csv\n",
      "lr                             0.008\n",
      "lr_decay                       0.8\n",
      "n_classes                      4\n",
      "n_folds                        10\n",
      "original_dataset_dir           ../data/Dataset/\n",
      "preprocess                     True\n",
      "preprocessed_dataset_dir       ../data/Preprocessed_Dataset/\n",
      "reduce_lr_patience             25\n",
      "regularization                 0.0\n",
      "save_best_model                True\n",
      "save_dir                       ../models\n",
      "self_supervised                True\n",
      "self_supervised_model_path     ../models/ss_models/feature_less_4_transformation_model3(batch2,rotation45).pth.tar\n",
      "tensorboard_dir                ../log_dir/runs\n",
      "test_size                      0.1\n",
      "verbose                        True\n",
      "verbose_epoch                  20\n",
      "xavier_initialization          False\n",
      "\n",
      "\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "KxCpZbuyEnXN"
   },
   "source": [
    "class DementiaDataset(Dataset):\n",
    "    \"\"\"Dementia Dataset\"\"\"\n",
    "\n",
    "    def __init__(self, original_dataset_dir, preprocessed_dataset_dir, labels_file, begin_at_slice=6, num_slices=7, image_crop_size=128,\n",
    "                 label_type='PVWM', preprocess=True, transform=None):\n",
    "\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            param original_dataset_dir (string): Path to the directory with all the original images.\n",
    "            param preprocessedd_dataset_dir (string): Path to the directory with all the preprocessed images,\n",
    "            param labels_file (string): Path to the csv file with PVWM and DWM labels.\n",
    "            begin_at_slice (int): The first slice number to use. This is because we are interested in middle slices.\n",
    "            num_slices (int): Number of slices from each MRI serie to use.\n",
    "            image_crop_size (int): The height and width of a center patch extracted from each original image.\n",
    "            label_type (string): Assumes values 'PVWM' or 'DWM'.\n",
    "            preprocess (boolean): Whether to preprocess samples.\n",
    "            transform (callable, optional): Optional transform to be applied on a sample.\n",
    "        \"\"\"\n",
    "\n",
    "        assert label_type in ['PVWM', 'DWM'], \"Invalid label type {}, label type must be one of ['PVWM', 'DWM']\".format(label_type)\n",
    "\n",
    "        self.original_dataset_dir = original_dataset_dir\n",
    "        self.preprocessed_dataset_dir = preprocessed_dataset_dir\n",
    "        self.labels = pd.read_csv(labels_file, dtype={\"ID\": str})\n",
    "        self.begin_at_slice = begin_at_slice\n",
    "        self.num_slices = num_slices\n",
    "        self.image_size = image_crop_size\n",
    "        self.label_type = label_type\n",
    "        self.preprocess = preprocess\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        label = self.labels.loc[idx, self.label_type]\n",
    "\n",
    "        try:\n",
    "            slices = self.get_original_slices(idx)\n",
    "        except Exception:\n",
    "            slices = self.get_original_modified_slices(idx)\n",
    "\n",
    "        if self.preprocess:\n",
    "            preprocessed_slices = self.get_preprocessed_slices(idx)\n",
    "            slices = torch.cat([slices, preprocessed_slices], dim=0)\n",
    "\n",
    "        sample = {'slices': slices, 'label': label}\n",
    "        return sample\n",
    "\n",
    "    def get_original_slices(self, idx):\n",
    "        slices = torch.zeros((self.num_slices, 1, self.image_size, self.image_size))\n",
    "\n",
    "        patient_id = self.labels.loc[idx, 'ID'] # patient id in the dataset\n",
    "        patient_dir = os.path.join(self.original_dataset_dir, patient_id)\n",
    "\n",
    "        all_slices = sorted(os.listdir(patient_dir))\n",
    "        middle_slices = all_slices[self.begin_at_slice: self.begin_at_slice + self.num_slices]\n",
    "\n",
    "        for i, slice_file in enumerate(middle_slices):\n",
    "            slice_file_path = os.path.join(patient_dir, slice_file)\n",
    "            slice_data = pydicom.dcmread(slice_file_path).pixel_array.astype(float)\n",
    "            slice_data *= 255.0 / np.max(slice_data) # normalize all images to [0-255]\n",
    "            slice_data = slice_data.astype(np.uint8)\n",
    "\n",
    "            if self.transform:\n",
    "                slice_data = self.transform(np.expand_dims(slice_data, 2))\n",
    "\n",
    "            slices[i] = slice_data\n",
    "        return slices\n",
    "\n",
    "    def get_preprocessed_slices(self, idx):\n",
    "        slices = torch.zeros((self.num_slices, 1, self.image_size, self.image_size))\n",
    "\n",
    "        patient_id = self.labels.loc[idx, 'ID'] # patient id in the dataset\n",
    "        patient_dir = os.path.join(self.preprocessed_dataset_dir, patient_id)\n",
    "\n",
    "        all_slices = sorted(os.listdir(patient_dir))\n",
    "        middle_slices = all_slices[self.begin_at_slice: self.begin_at_slice + self.num_slices]\n",
    "\n",
    "        for i, slice_file in enumerate(middle_slices):\n",
    "            slice_file_path = os.path.join(patient_dir, slice_file)\n",
    "\n",
    "            slice_data = io.imread(slice_file_path)\n",
    "            slice_data = (slice_data * (255.0 / np.max(slice_data))) # normalize all images to [0-255]\n",
    "            slice_data = slice_data.astype(np.uint8)\n",
    "\n",
    "            if self.transform:\n",
    "                slice_data = self.transform(np.expand_dims(slice_data, 2))\n",
    "\n",
    "            slices[i] = slice_data\n",
    "        return slices\n",
    "\n",
    "    def get_original_modified_slices(self, idx):\n",
    "        slices = torch.zeros((self.num_slices, 1, self.image_size, self.image_size))\n",
    "\n",
    "        patient_id = self.labels.loc[idx, 'ID'] # patient id in the dataset\n",
    "        patient_dir = os.path.join(self.original_dataset_dir, patient_id)\n",
    "\n",
    "        all_slices = sorted(os.listdir(patient_dir))\n",
    "        middle_slices = all_slices[self.begin_at_slice: self.begin_at_slice + self.num_slices]\n",
    "\n",
    "        for i, slice_file in enumerate(middle_slices):\n",
    "            slice_file_path = os.path.join(patient_dir, slice_file)\n",
    "\n",
    "            slice_data = io.imread(slice_file_path)\n",
    "            slice_data = (slice_data * (255.0 / np.max(slice_data))) # normalize all images to [0-255]\n",
    "            slice_data = slice_data.astype(np.uint8)\n",
    "\n",
    "            if self.transform:\n",
    "                slice_data = self.transform(np.expand_dims(slice_data, 2))\n",
    "\n",
    "            slices[i] = slice_data\n",
    "        return slices"
   ],
   "execution_count": 4,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pBYM73_nc6t0"
   },
   "source": [
    "dataset = DementiaDataset(original_dataset_dir=config.original_dataset_dir,\n",
    "                          preprocessed_dataset_dir=config.preprocessed_dataset_dir,\n",
    "                          labels_file=config.labels_file,\n",
    "                          label_type=config.label_type,\n",
    "                          preprocess=config.preprocess,\n",
    "                          begin_at_slice=config.dataset_begin_at_slice,\n",
    "                          num_slices=config.dataset_num_slice, \n",
    "                          image_crop_size=config.image_crop_size\n",
    "                          )"
   ],
   "execution_count": 5,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "WZ64eKBvgXFQ"
   },
   "source": [
    "def get_data_loaders(dataset, train_indices, val_indices, test_indices, batch_size=config.batch_size,\n",
    "                     train_transform=config.train_transform, validation_transform=config.validation_transform, test_transform=config.validation_transform):\n",
    "    \n",
    "    train_set = Subset(dataset, train_indices)\n",
    "    val_set = Subset(dataset, val_indices)\n",
    "    test_set = Subset(dataset, test_indices)\n",
    "\n",
    "    train_set.dataset = copy(dataset)\n",
    "\n",
    "    train_set.dataset.transform = train_transform\n",
    "    val_set.dataset.transform = validation_transform\n",
    "    test_set.dataset.transform = test_transform\n",
    "\n",
    "    train_targets = torch.from_numpy(dataset.labels[dataset.label_type].values[train_set.indices])\n",
    "    _, train_class_sample_counts = torch.unique(train_targets, sorted=True, return_counts=True)\n",
    "    train_weights = 1. / train_class_sample_counts.float()\n",
    "    train_samples_weights = train_weights[train_targets]\n",
    "    train_sampler = WeightedRandomSampler(weights=train_samples_weights, num_samples=len(train_samples_weights), replacement=True)\n",
    "\n",
    "    train_loader = DataLoader(train_set, batch_size=batch_size, sampler=train_sampler)\n",
    "    validation_loader = DataLoader(val_set, batch_size=batch_size)\n",
    "    test_loader = DataLoader(test_set, batch_size=batch_size)\n",
    "\n",
    "    return train_loader, validation_loader, test_loader"
   ],
   "execution_count": 6,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "b3O6M8ehJUSR"
   },
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, n_classes=config.n_classes, resnet_unet_path=None, L=64, D=16, K=1):\n",
    "        super(Model, self).__init__()\n",
    "\n",
    "        self.L = L\n",
    "        self.D = D\n",
    "        self.K = K\n",
    "\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "        self.attention = nn.Sequential(\n",
    "            nn.Linear(self.L, self.D),\n",
    "            nn.Tanh(),\n",
    "            nn.Linear(self.D, self.K)\n",
    "        )\n",
    "\n",
    "\n",
    "        self.classifier = nn.Sequential(\n",
    "            nn.Linear(self.L * self.K, self.n_classes),\n",
    "            nn.LogSoftmax(dim=1)\n",
    "        )\n",
    "\n",
    "        self.resnet_unet = None\n",
    "\n",
    "\n",
    "        if resnet_unet_path:\n",
    "            map_location = torch.device('cuda')\n",
    "            if not config.cuda:\n",
    "                map_location = torch.device('cpu')\n",
    "            checkpoint = torch.load(resnet_unet_path, map_location=map_location)\n",
    "            resnet_unet = ResNetUNet(n_class=1)\n",
    "            resnet_unet.load_state_dict(checkpoint['state_dict'])\n",
    "            self.resnet_unet = resnet_unet\n",
    "\n",
    "\n",
    "        if self.resnet_unet:\n",
    "            self.init_with_self_supervised_model()\n",
    "\n",
    "        else: \n",
    "            self.init_basic_model()\n",
    "\n",
    "\n",
    "    def init_basic_model(self):\n",
    "        \n",
    "        self.feature_extractor_part1 = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('conv1', nn.Conv2d(1, 2, kernel_size=3, stride=1)),\n",
    "                ('relu1', nn.ReLU()),\n",
    "                ('pool1', nn.MaxPool2d(2)),\n",
    "\n",
    "                ('conv2', nn.Conv2d(2, 4, kernel_size=3, stride=1)),\n",
    "                ('relu2', nn.ReLU()),\n",
    "                ('pool2', nn.MaxPool2d(2)),\n",
    "\n",
    "                ('conv3', nn.Conv2d(4, 6, kernel_size=3, stride=1)),\n",
    "                ('relu3', nn.ReLU()),\n",
    "                ('pool3', nn.MaxPool2d(2)),\n",
    "\n",
    "                ('conv4', nn.Conv2d(6, 8, kernel_size=3, stride=1)),\n",
    "                ('relu4', nn.ReLU()),\n",
    "                ('pool4', nn.MaxPool2d(2))])\n",
    "        )\n",
    "\n",
    "        self.feature_extractor_part2 = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('linear', nn.Linear(8 * 6 * 6, self.L)),\n",
    "                ('relu', nn.ReLU()),\n",
    "            ])\n",
    "        )\n",
    "\n",
    "\n",
    "  \n",
    "    def init_with_self_supervised_model(self):\n",
    "\n",
    "        self.feature_extractor_part1 = nn.Sequential(self.resnet_unet)\n",
    "\n",
    "        self.feature_extractor_part2 = nn.Sequential(\n",
    "            OrderedDict([\n",
    "                ('linear', nn.Linear(16 * 4 * 4, self.L)),\n",
    "                ('relu', nn.ReLU()),\n",
    "            ])\n",
    "        )\n",
    "        \n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.squeeze(0)\n",
    "\n",
    "        H = self.feature_extractor_part1(x)\n",
    "        H = H.view(-1, self.feature_extractor_part2.linear.in_features)\n",
    "        H = self.feature_extractor_part2(H)  # NxL\n",
    "\n",
    "        A = self.attention(H)  # NxK\n",
    "        A = torch.transpose(A, 1, 0)  # KxN\n",
    "        A = F.softmax(A, dim=1)\n",
    "\n",
    "\n",
    "        M = torch.mm(A, H)  # KxL\n",
    "\n",
    "        Y_prob = self.classifier(M)\n",
    "        Y_hat = torch.argmax(Y_prob)\n",
    "        \n",
    "        return Y_prob, Y_hat, A"
   ],
   "execution_count": 7,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "mBOVYUIO-snU"
   },
   "source": [
    "# Function for Xavier initialization of network weights.\n",
    "\n",
    "def weights_init(m):\n",
    "    if isinstance(m, nn.Conv2d) or isinstance(m, nn.Linear):\n",
    "        print(m)\n",
    "        nn.init.xavier_uniform_(m.weight)\n",
    "        nn.init.constant_(m.bias, 0)"
   ],
   "execution_count": 8,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "aYNvx_Hn-n7O",
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1612009691706,
     "user_tz": -210,
     "elapsed": 12773,
     "user": {
      "displayName": "Reza Shirkavand",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPyXXp5NbGLthkrc4vSuHR0QVQAt42j8nJsGQr=s64",
      "userId": "01746781096781078766"
     }
    },
    "outputId": "c75775c9-c054-4135-9f38-023b3117dbe8"
   },
   "source": [
    "# Test everything is fine with the forward pass of our model. Also view number of total parameters as well as trainable parameters of the model.\n",
    "\n",
    "model = Model(resnet_unet_path=config.self_supervised_model_path)\n",
    "\n",
    "# model.apply(weights_init)\n",
    "\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "print(f'{total_params:,} total parameters.')\n",
    "total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "print(f'{total_trainable_params:,} training parameters.')\n",
    "inp = torch.rand([8, 1, 128, 128])\n",
    "print(model(inp)[0].shape)"
   ],
   "execution_count": 9,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "63,912 total parameters.\n",
      "63,912 training parameters.\n",
      "torch.Size([1, 4])\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "IS2xzDTYiOuv"
   },
   "source": [
    "resnet_unet_path = config.self_supervised_model_path\n",
    "def get_model(cuda=config.cuda, self_supervised=config.self_supervised, resnet_unet_path=resnet_unet_path, xavier=config.xavier_initialization):\n",
    "\n",
    "    print('Init Model')\n",
    "    \n",
    "    if self_supervised:\n",
    "        model = Model(resnet_unet_path=resnet_unet_path)\n",
    "        \n",
    "    else:\n",
    "        model = Model()\n",
    "\n",
    "    if xavier:\n",
    "        model.apply(weights_init)\n",
    "\n",
    "\n",
    "    if cuda:\n",
    "        model.cuda()\n",
    "\n",
    "    total_params = sum(p.numel() for p in model.parameters())\n",
    "    print(f'{total_params:,} total parameters.')\n",
    "    total_trainable_params = sum(\n",
    "    p.numel() for p in model.parameters() if p.requires_grad)\n",
    "    print(f'{total_trainable_params:,} training parameters.')\n",
    "\n",
    "    return model"
   ],
   "execution_count": 10,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "pWiSWNyUqzsO"
   },
   "source": [
    "import shutil\n",
    "try:\n",
    "    shutil.rmtree(config.tensorboard_dir)\n",
    "except FileNotFoundError:\n",
    "    pass"
   ],
   "execution_count": 11,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "SrNR7LmAn1X9"
   },
   "source": [
    "from torch.utils.tensorboard import SummaryWriter\n",
    "\n",
    "writer = SummaryWriter('{}/Dementia_{}_Biomarker_Prediction'.format(config.tensorboard_dir, dataset.label_type))"
   ],
   "execution_count": 12,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "kLOWaTFuU8lm"
   },
   "source": [
    "from textwrap import wrap\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "\n",
    "\n",
    "def plot_confusion_matrix(y_true, y_pred, labels, title='Confusion matrix'):\n",
    "    ''' \n",
    "    Parameters:\n",
    "        y_true: These are your true classification categories.\n",
    "        y_pred: These are you predicted classification categories\n",
    "        labels: This is a lit of labels which will be used to display the axis labels\n",
    "        title='Confusion matrix': Title for your matrix\n",
    "\n",
    "    Returns:\n",
    "        fig: the figure of confusion matrix \n",
    "    '''\n",
    "    cm = confusion_matrix(y_true, y_pred, labels=labels)\n",
    "\n",
    "    np.set_printoptions(precision=2)\n",
    "\n",
    "    fig = plt.figure(figsize=(7, 7), dpi=320, facecolor='w', edgecolor='k')\n",
    "    ax = fig.add_subplot(1, 1, 1)\n",
    "    im = ax.imshow(cm, cmap='Oranges')\n",
    "\n",
    "    classes = labels\n",
    "    classes = ['\\n'.join(wrap(str(l), 40)) for l in classes]\n",
    "\n",
    "    tick_marks = np.arange(len(classes))\n",
    "\n",
    "    ax.set_xlabel('Predicted', fontsize=7)\n",
    "    ax.set_xticks(tick_marks)\n",
    "    c = ax.set_xticklabels(classes, fontsize=4, rotation=-90,  ha='center')\n",
    "    ax.xaxis.set_label_position('bottom')\n",
    "    ax.xaxis.tick_bottom()\n",
    "\n",
    "    ax.set_ylabel('True Label', fontsize=7)\n",
    "    ax.set_yticks(tick_marks)\n",
    "    ax.set_yticklabels(classes, fontsize=4, va ='center')\n",
    "    ax.yaxis.set_label_position('left')\n",
    "    ax.yaxis.tick_left()\n",
    "\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        ax.text(j, i, format(cm[i, j], 'd') if cm[i,j]!=0 else '.', horizontalalignment=\"center\", fontsize=6, verticalalignment='center', color= \"black\")\n",
    "\n",
    "    fig.set_tight_layout(True)\n",
    "    \n",
    "    return fig"
   ],
   "execution_count": 20,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "FPKjaO12jHOx"
   },
   "source": [
    "def plot_instance_bag(split, data_loader):\n",
    "    data_iter = iter(data_loader)\n",
    "    sample = data_iter.next()\n",
    "\n",
    "    images = sample['slices'][0]\n",
    "    label = sample['label']\n",
    "\n",
    "    img_grid = torchvision.utils.make_grid(images)\n",
    "\n",
    "    writer.add_image('A bag of class {} instances/{}'.format(label, split), img_grid)"
   ],
   "execution_count": 21,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "R2kdd3FD4R2s"
   },
   "source": [
    "def save_checkpoint(state, split, epoch, save_path=config.save_dir):\n",
    "    filename = os.path.join(save_path, 'best_{}_model_epoch_{}_split_{}.pth.tar'.format(dataset.label_type, epoch, split))\n",
    "    torch.save(state, filename)"
   ],
   "execution_count": 22,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "1h8OC0rAz9YH"
   },
   "source": [
    "def report(phase, loss, y_true, y_pred, split, epoch, save_model=config.save_best_model):\n",
    "\n",
    "    global best_score\n",
    "\n",
    "    accuracy = accuracy_score(y_true, y_pred)\n",
    "\n",
    "    writer.add_scalar('Loss/{}-{}'.format(phase, split), loss.item(), epoch)\n",
    "    writer.add_scalar('Accuracy/{}-{}'.format(phase, split), accuracy, epoch)\n",
    "\n",
    "    matrix = confusion_matrix(y_true, y_pred)\n",
    "    class_accuracy = matrix.diagonal()/matrix.sum(axis=1)\n",
    "    class_accuracy_dict = dict(enumerate(class_accuracy))\n",
    "    class_accuracy_dict = {str(k):v for k,v in class_accuracy_dict.items()}\n",
    "    writer.add_scalars('Class Accuracy/{}-{}'.format(phase, split), class_accuracy_dict, epoch)\n",
    "\n",
    "\n",
    "    macro_f1_score = f1_score(y_true, y_pred, average='macro')\n",
    "    micro_f1_score = f1_score(y_true, y_pred, average='micro')\n",
    "\n",
    "    writer.add_scalar('Macro F-1 Score/{}-{}'.format(phase, split), macro_f1_score, epoch)\n",
    "    writer.add_scalar('Micro F-1 Score/{}-{}'.format(phase, split), micro_f1_score, epoch)\n",
    "\n",
    "    if epoch % config.verbose_epoch == 0:\n",
    "        print('--------------------------------')\n",
    "        print('Split: {}, Epoch: {}, Phase: {}, Loss: {:.4f}, accuracy: {:.4f}'.format(split, epoch, phase.upper(), loss.item(), accuracy))\n",
    "        print('{} class accuracy: {}'.format(phase, class_accuracy))\n",
    "        \n",
    "        if phase == 'Validation':\n",
    "            print('\\n\\n')\n",
    "\n",
    "\n",
    "    if phase == 'Validation':\n",
    "        is_best = macro_f1_score >= best_score\n",
    "        best_score = max(best_score, macro_f1_score)\n",
    "        \n",
    "        if is_best:\n",
    "            if save_model and best_score > 0.8:\n",
    "                save_checkpoint({\n",
    "                    'epoch': epoch,\n",
    "                    'state_dict': model.state_dict(),\n",
    "                    'best_score': best_score,\n",
    "                    'optimizer' : optimizer.state_dict(),\n",
    "                }, split, epoch)\n",
    "            conf_matrix_plot = plot_confusion_matrix(y_true, y_pred, np.arange(n_classes))\n",
    "            writer.add_figure('Confusion Matrix/{}-{}'.format(phase, split), conf_matrix_plot, epoch)\n",
    "            writer.add_scalar('Best Score/{}-{}'.format(phase, split), best_score, epoch)\n",
    "            writer.add_scalar('Best Acc/{}-{}'.format(phase, split), accuracy, epoch)"
   ],
   "execution_count": 23,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "J7CNHEUFKNk5"
   },
   "source": [
    "def train(split, epoch):\n",
    "    model.train()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    train_loss = 0.0\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    \n",
    "    for batch_idx, sample in enumerate(train_loader):\n",
    "        data = sample['slices']\n",
    "        bag_label = sample['label']\n",
    "\n",
    "        if cuda:\n",
    "            data, bag_label = data.cuda(), bag_label.cuda()\n",
    "\n",
    "        y_prob, y_hat, _ = model(data)\n",
    "\n",
    "        loss = criterion(y_prob, bag_label)\n",
    "        train_loss += loss\n",
    "\n",
    "        y_true.append(bag_label[0].item())\n",
    "        y_pred.append(y_hat.item())\n",
    "\n",
    "    train_loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "\n",
    "    train_loss /= len(y_true)\n",
    "    \n",
    "    report(phase='Training', loss=train_loss, y_true=y_true, y_pred=y_pred, split=split, epoch=epoch)\n",
    "\n",
    "    validate(split, epoch)"
   ],
   "execution_count": 24,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "Ilbm0VtDKo_F"
   },
   "source": [
    "def validate(split, epoch):\n",
    "\n",
    "    model.eval()\n",
    "\n",
    "    y_true = []\n",
    "    y_pred = []\n",
    "\n",
    "    val_loss = 0.0\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for batch_idx, sample in enumerate(validation_loader):\n",
    "            data = sample['slices']\n",
    "            bag_label = sample['label']\n",
    "            if cuda:\n",
    "                data, bag_label = data.cuda(), bag_label.cuda()\n",
    "        \n",
    "            y_prob, y_hat, _ = model(data)\n",
    "\n",
    "            loss = criterion(y_prob, bag_label)\n",
    "            val_loss += loss\n",
    "\n",
    "            y_true.append(bag_label[0].item())\n",
    "            y_pred.append(y_hat.item())\n",
    "\n",
    "    val_loss /= len(y_true)\n",
    "\n",
    "    lr_scheduler.step(val_loss)\n",
    "\n",
    "    report(phase='Validation', loss=val_loss, y_true=y_true, y_pred=y_pred, split=split, epoch=epoch)"
   ],
   "execution_count": 25,
   "outputs": []
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "7fbHGcZe7EAX",
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 821
    },
    "executionInfo": {
     "status": "ok",
     "timestamp": 1612009918784,
     "user_tz": -210,
     "elapsed": 4422,
     "user": {
      "displayName": "Reza Shirkavand",
      "photoUrl": "https://lh3.googleusercontent.com/a-/AOh14GiPyXXp5NbGLthkrc4vSuHR0QVQAt42j8nJsGQr=s64",
      "userId": "01746781096781078766"
     }
    },
    "outputId": "39968949-abb3-43ee-da89-8b1af3292664"
   },
   "source": [
    "%tensorboard --logdir=log_dir/runs"
   ],
   "execution_count": 50,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "UsageError: Line magic function `%tensorboard` not found.\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "ukOc8DD4gEVk"
   },
   "source": [
    "from sklearn.model_selection import KFold\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "kf = KFold(n_splits=config.n_folds, shuffle=True)\n",
    "dataset_size = len(dataset)\n",
    "\n",
    "best_scores = []\n",
    "\n",
    "cuda = False\n",
    "if cuda:\n",
    "    print('\\nGPU is ON!')\n",
    "\n",
    "indices = np.arange(dataset_size)\n",
    "train_indices, test_indices = train_test_split(indices, test_size=config.test_size)\n",
    "\n",
    "for split, (train_indices, val_indices) in enumerate(kf.split(indices)):\n",
    "    \n",
    "    print('\\n\\n\\n-----------SPLIT {} ------------\\n'.format(split))\n",
    "\n",
    "    batch_size = config.batch_size\n",
    "    lr = config.lr\n",
    "    epochs = config.epochs\n",
    "    verbose_epoch = config.verbose_epoch\n",
    "    weight_decay = config.regularization\n",
    "    patience = config.reduce_lr_patience\n",
    "    lr_decay = config.lr_decay\n",
    "\n",
    "    n_classes = np.unique(dataset.labels[dataset.label_type].values).shape[0]\n",
    "\n",
    "    train_loader, validation_loader, test_loader = get_data_loaders(dataset=dataset, batch_size=batch_size,\n",
    "                                                                    train_indices=train_indices, val_indices=val_indices, test_indices=val_indices)\n",
    "\n",
    "    plot_instance_bag(split, train_loader)\n",
    "\n",
    "    model = get_model(cuda=cuda, self_supervised=config.self_supervised)\n",
    "    optimizer = optim.Adam(model.parameters(), lr=lr, betas=(0.9, 0.999), weight_decay=weight_decay)\n",
    "    criterion = nn.NLLLoss()\n",
    "    lr_scheduler = ReduceLROnPlateau(optimizer, \"min\", factor=lr_decay, patience=patience, verbose=config.verbose)\n",
    "\n",
    "    best_score = 0.0\n",
    "\n",
    "    print('Start Training')\n",
    "\n",
    "    for epoch in range(1, epochs):\n",
    "        train(split, epoch)\n",
    "    \n",
    "    best_scores.append(best_score)\n",
    "\n"
   ],
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "-----------SPLIT 0 ------------\n",
      "\n",
      "Init Model\n",
      "63,912 total parameters.\n",
      "63,912 training parameters.\n",
      "Start Training\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reza/PycharmProjects/Medical-Imaging/venv/lib/python3.7/site-packages/ipykernel_launcher.py:11: RuntimeWarning: invalid value encountered in true_divide\n",
      "  # This is added back by InteractiveShellApp.init_path()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------\n",
      "Split: 0, Epoch: 20, Phase: TRAINING, Loss: 0.5737, accuracy: 0.7778\n",
      "Training class accuracy: [1.  0.5 1.  0. ]\n",
      "--------------------------------\n",
      "Split: 0, Epoch: 20, Phase: VALIDATION, Loss: 0.1162, accuracy: 1.0000\n",
      "Validation class accuracy: [1.]\n",
      "\n",
      "\n",
      "\n",
      "--------------------------------\n",
      "Split: 0, Epoch: 40, Phase: TRAINING, Loss: 0.2535, accuracy: 0.8889\n",
      "Training class accuracy: [0.67 1.   1.  ]\n",
      "--------------------------------\n",
      "Split: 0, Epoch: 40, Phase: VALIDATION, Loss: 3.4138, accuracy: 0.0000\n",
      "Validation class accuracy: [ 0. nan]\n",
      "\n",
      "\n",
      "\n",
      "Epoch    45: reducing learning rate of group 0 to 6.4000e-03.\n",
      "--------------------------------\n",
      "Split: 0, Epoch: 60, Phase: TRAINING, Loss: 0.4022, accuracy: 0.7778\n",
      "Training class accuracy: [0. 1. 1. 1.]\n",
      "--------------------------------\n",
      "Split: 0, Epoch: 60, Phase: VALIDATION, Loss: 2.8272, accuracy: 0.0000\n",
      "Validation class accuracy: [ 0. nan]\n",
      "\n",
      "\n",
      "\n",
      "Epoch    71: reducing learning rate of group 0 to 5.1200e-03.\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m                         Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-26-90bcfb9fd758>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     44\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     45\u001B[0m     \u001B[0;32mfor\u001B[0m \u001B[0mepoch\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mrange\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;36m1\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepochs\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 46\u001B[0;31m         \u001B[0mtrain\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0msplit\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mepoch\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     47\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     48\u001B[0m     \u001B[0mbest_scores\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mappend\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mbest_score\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-24-f1ee5cd4e72f>\u001B[0m in \u001B[0;36mtrain\u001B[0;34m(split, epoch)\u001B[0m\n\u001B[1;32m      9\u001B[0m     \u001B[0moptimizer\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mzero_grad\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 11\u001B[0;31m     \u001B[0;32mfor\u001B[0m \u001B[0mbatch_idx\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msample\u001B[0m \u001B[0;32min\u001B[0m \u001B[0menumerate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtrain_loader\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     12\u001B[0m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msample\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'slices'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     13\u001B[0m         \u001B[0mbag_label\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0msample\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0;34m'label'\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Medical-Imaging/venv/lib/python3.7/site-packages/torch/utils/data/dataloader.py\u001B[0m in \u001B[0;36m__next__\u001B[0;34m(self)\u001B[0m\n\u001B[1;32m    344\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__next__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    345\u001B[0m         \u001B[0mindex\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_next_index\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 346\u001B[0;31m         \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_dataset_fetcher\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mindex\u001B[0m\u001B[0;34m)\u001B[0m  \u001B[0;31m# may raise StopIteration\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    347\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_pin_memory\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    348\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0m_utils\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpin_memory\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mdata\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Medical-Imaging/venv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36mfetch\u001B[0;34m(self, possibly_batched_index)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Medical-Imaging/venv/lib/python3.7/site-packages/torch/utils/data/_utils/fetch.py\u001B[0m in \u001B[0;36m<listcomp>\u001B[0;34m(.0)\u001B[0m\n\u001B[1;32m     42\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0mfetch\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     43\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mauto_collation\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 44\u001B[0;31m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;32mfor\u001B[0m \u001B[0midx\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     45\u001B[0m         \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     46\u001B[0m             \u001B[0mdata\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mpossibly_batched_index\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Medical-Imaging/venv/lib/python3.7/site-packages/torch/utils/data/dataset.py\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m    255\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    256\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__getitem__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0midx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 257\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataset\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mindices\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    258\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    259\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__len__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-4-7eb947fc5b0e>\u001B[0m in \u001B[0;36m__getitem__\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     37\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     38\u001B[0m         \u001B[0;32mtry\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 39\u001B[0;31m             \u001B[0mslices\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_original_slices\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     40\u001B[0m         \u001B[0;32mexcept\u001B[0m \u001B[0mException\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     41\u001B[0m             \u001B[0mslices\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_original_modified_slices\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0midx\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m<ipython-input-4-7eb947fc5b0e>\u001B[0m in \u001B[0;36mget_original_slices\u001B[0;34m(self, idx)\u001B[0m\n\u001B[1;32m     64\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     65\u001B[0m             \u001B[0;32mif\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 66\u001B[0;31m                 \u001B[0mslice_data\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mnp\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexpand_dims\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mslice_data\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;36m2\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     67\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     68\u001B[0m             \u001B[0mslices\u001B[0m\u001B[0;34m[\u001B[0m\u001B[0mi\u001B[0m\u001B[0;34m]\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mslice_data\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Medical-Imaging/venv/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m     68\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__call__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     69\u001B[0m         \u001B[0;32mfor\u001B[0m \u001B[0mt\u001B[0m \u001B[0;32min\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransforms\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 70\u001B[0;31m             \u001B[0mimg\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mt\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     71\u001B[0m         \u001B[0;32mreturn\u001B[0m \u001B[0mimg\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     72\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Medical-Imaging/venv/lib/python3.7/site-packages/torchvision/transforms/transforms.py\u001B[0m in \u001B[0;36m__call__\u001B[0;34m(self, img)\u001B[0m\n\u001B[1;32m    998\u001B[0m         \u001B[0mangle\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_params\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdegrees\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    999\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 1000\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mF\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrotate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mangle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mresample\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexpand\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mcenter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1001\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1002\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0m__repr__\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Medical-Imaging/venv/lib/python3.7/site-packages/torchvision/transforms/functional.py\u001B[0m in \u001B[0;36mrotate\u001B[0;34m(img, angle, resample, expand, center)\u001B[0m\n\u001B[1;32m    712\u001B[0m         \u001B[0;32mraise\u001B[0m \u001B[0mTypeError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'img should be PIL Image. Got {}'\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mformat\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mtype\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    713\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 714\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mimg\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mrotate\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mangle\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresample\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mexpand\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mcenter\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    715\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    716\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Medical-Imaging/venv/lib/python3.7/site-packages/PIL/Image.py\u001B[0m in \u001B[0;36mrotate\u001B[0;34m(self, angle, resample, expand, center, translate, fillcolor)\u001B[0m\n\u001B[1;32m   2003\u001B[0m             \u001B[0mw\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnw\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mnh\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2004\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2005\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mtransform\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mw\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mh\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mAFFINE\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mmatrix\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mresample\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfillcolor\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mfillcolor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2006\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2007\u001B[0m     \u001B[0;32mdef\u001B[0m \u001B[0msave\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfp\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mformat\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0;32mNone\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0;34m**\u001B[0m\u001B[0mparams\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Medical-Imaging/venv/lib/python3.7/site-packages/PIL/Image.py\u001B[0m in \u001B[0;36mtransform\u001B[0;34m(self, size, method, data, resample, fill, fillcolor)\u001B[0m\n\u001B[1;32m   2297\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"missing method data\"\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2298\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2299\u001B[0;31m         \u001B[0mim\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mnew\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mself\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfillcolor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2300\u001B[0m         \u001B[0;32mif\u001B[0m \u001B[0mmethod\u001B[0m \u001B[0;34m==\u001B[0m \u001B[0mMESH\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2301\u001B[0m             \u001B[0;31m# list of quads\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/PycharmProjects/Medical-Imaging/venv/lib/python3.7/site-packages/PIL/Image.py\u001B[0m in \u001B[0;36mnew\u001B[0;34m(mode, size, color)\u001B[0m\n\u001B[1;32m   2487\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0mcolor\u001B[0m \u001B[0;32mis\u001B[0m \u001B[0;32mNone\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2488\u001B[0m         \u001B[0;31m# don't initialize\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m-> 2489\u001B[0;31m         \u001B[0;32mreturn\u001B[0m \u001B[0mImage\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0m_new\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcore\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mnew\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmode\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msize\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   2490\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   2491\u001B[0m     \u001B[0;32mif\u001B[0m \u001B[0misStringType\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mcolor\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mKeyboardInterrupt\u001B[0m: "
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "metadata": {
    "id": "luDyBkH7p_cu"
   },
   "source": [
    "print(np.mean(best_scores))"
   ],
   "execution_count": 27,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nan\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/reza/PycharmProjects/Medical-Imaging/venv/lib/python3.7/site-packages/numpy/core/fromnumeric.py:3257: RuntimeWarning: Mean of empty slice.\n",
      "  out=out, **kwargs)\n",
      "/Users/reza/PycharmProjects/Medical-Imaging/venv/lib/python3.7/site-packages/numpy/core/_methods.py:161: RuntimeWarning: invalid value encountered in double_scalars\n",
      "  ret = ret.dtype.type(ret / rcount)\n"
     ]
    }
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ]
}